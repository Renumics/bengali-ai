{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8eeb0-7d25-4294-9df2-d5786d40b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a9143f-f4d2-4e96-bf95-e08449e789ef",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Bengali.AI Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18586a-ab0d-4ac0-b568-25f39e88e1d2",
   "metadata": {},
   "source": [
    "This Notebook aims at doing a brief **exploratory data exploration** of the data available for the Bengali.AI Competition. Concretely by going through this notebook you will get the following:\n",
    "1. **Insights** on the available training and validation data (basic information, biases, outliers, duplicates, ...)\n",
    "2. A **template for doing interactive EDA** in the EDA tool [Spotlight](https://github.com/Renumics/spotlight)\n",
    "3. An **embedding- and feature-enriched dataset** as a starting point for your own analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea554b-8d6d-4466-a01e-7979bee3504c",
   "metadata": {},
   "source": [
    "**NOTE**: You have to **adjust the path to your dataset** in the **Setup section** below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15d40d-7a70-4678-9458-589228b69680",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682899d-c074-4083-acc6-1ee91923e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these dependencies\n",
    "!pip install -U sliceguard pandas numpy plotly datasets scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461abdf-098f-4bd8-b7fe-0c71b729e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the path to your dataset here\n",
    "INPUT_DIR = \"/home/daniel/data/bengaliai/bengaliai-speech\" # CHANGE DATASET PATH HERE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cfe72-5e0b-4895-ac18-75ee74393979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports you will need\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import plotly.express as px\n",
    "from sklearn.neighbors import KDTree\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Audio, Embedding\n",
    "from sliceguard import SliceGuard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb7ff4-c4f9-457f-ae01-390938fe364c",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "This code is simply for loading the data. *df* will afterwards contain a feature- and embedding-enriched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bc5ae-c0c4-49e4-9107-b982b7a68e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from your machine\n",
    "df = pd.read_csv(Path(INPUT_DIR) / \"train.csv\")\n",
    "\n",
    "# Pull features and embeddings from huggingface dataset hub\n",
    "dataset = datasets.load_dataset(\"renumics/bengaliai-competition-features-embeddings\")\n",
    "feature_df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Merge the two datasets\n",
    "additional_columns = feature_df.columns.difference(df.columns).tolist() + [\"id\"]\n",
    "\n",
    "df = pd.merge(df, feature_df[additional_columns], on='id')\n",
    "\n",
    "if not INPUT_DIR.endswith(\"/\"):\n",
    "    INPUT_DIR = INPUT_DIR + \"/\"\n",
    "df[\"audio\"] = INPUT_DIR + df[\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450cd12-ea4e-4b19-a735-f1f5b38d9fef",
   "metadata": {},
   "source": [
    "## Basics about the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff6630-b653-432b-860c-4c4ef2de6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample count and columns\n",
    "print(f\"Sample count is {len(df)}.\")\n",
    "print(f\"Dataframe contains the columns {df.columns.tolist()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2e4a8-2fde-449e-b991-e7a901936803",
   "metadata": {},
   "source": [
    "**Dataframe Structure:**\n",
    "* The dataframe contains almost 1 million rows\n",
    "* It contains the following basic columns:\n",
    "    * *id*: The sample id which maps to the corresponding audio file in \"train_mp3s\"\n",
    "    * *sentence* The ground-truth transcription of the audio file \n",
    "* The enrichment adds the following columns:\n",
    "    * *audio_length_s*: Length of the audio file in seconds\n",
    "    * *audio_rms_max*: Maximum signal energy of the sample\n",
    "    * *audio_rms_mean*: Mean signal energy of the sample\n",
    "    * *audio_rms_std*: Maximum signal energy standard deviation\n",
    "    * *audio_spectral_flatness_mean*: Audio spectral flatness mean (the higher the more noise like)\n",
    "    * *audio_embedding*: Audio embeddings computed using embedding model trained on Audioset\n",
    "    * *text_embedding*: Multilingual text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b39540-dd23-4d93-bcb6-8fbe7c5266a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ratio\n",
    "print(\"##### Distribution between splits #####\")\n",
    "px.histogram(df, x=\"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9f94d-9717-4daf-beb8-55ff82e8f6fc",
   "metadata": {},
   "source": [
    "**Data Split Ratio:**\n",
    "* Around 30k samples of all public data are part of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0139f-8910-4696-a95b-2142a64621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the split is random or group-wise\n",
    "spotlight.show(df.groupby(\"split\").sample(3000), dtype={\"audio_embedding\": Embedding, \"text_embedding\": Embedding, \"audio\": Audio})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c20546-a8c2-4194-8099-6eeb92276d4d",
   "metadata": {},
   "source": [
    "*Result when ordered by audio_embedding*:\n",
    "\n",
    "![Split Image](images/split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd2dfd-5f05-4b44-b012-706e968204ce",
   "metadata": {},
   "source": [
    "**Which type of split is this?**\n",
    "* Seems like a not completely random sample-wise split.\n",
    "* Most of the audio embedding space covered by samples from both splits.\n",
    "* However, there are groups that only exist in the train split.\n",
    "* Additionally there are regions where the validation data is a lot denser, however to some extent exists in the train data.\n",
    "* So just beware that it could make sense to track specific regions in your evaluation and potentially adjust the sample distribution if your model is weak with certain types of samples!\n",
    "* Something similar is also observable for the text_embedding field, however the embedding quality is probably quite bad so the effect cannot be shown as strongly here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a29c4-0448-4cb5-9396-cff1359219a6",
   "metadata": {},
   "source": [
    "## Distribution of Simple Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15c5be-f01e-4411-b4af-a976552cda89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### Distribution of audio lengths (s) #####\")\n",
    "audio_length_fig = px.histogram(df, x=\"audio_length_s\", nbins=200)\n",
    "audio_length_fig.show()\n",
    "\n",
    "print(\"##### Distribution of rms (signal power) means #####\")\n",
    "audio_rms_means_fig = px.histogram(df, x=\"audio_rms_mean\", nbins=200)\n",
    "audio_rms_means_fig.show()\n",
    "\n",
    "print(\"##### Distribution of rms (signal power) maxs #####\")\n",
    "audio_rms_maxs_fig = px.histogram(df, x=\"audio_rms_max\", nbins=200)\n",
    "audio_rms_maxs_fig.show()\n",
    "\n",
    "print(\"##### Distribution of rms (signal power) stds #####\")\n",
    "audio_rms_stds_fig = px.histogram(df, x=\"audio_rms_std\", nbins=200)\n",
    "audio_rms_stds_fig.show()\n",
    "\n",
    "\n",
    "print(\"##### Distribution of spectral flatness (noisyness) #####\")\n",
    "audio_spectral_flatness_fig = px.histogram(df, x=\"audio_spectral_flatness_mean\", nbins=200)\n",
    "audio_spectral_flatness_fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b1571-5f94-4151-9b8c-ca82477d4673",
   "metadata": {},
   "source": [
    "**Simple Audio Feature Distributions:**\n",
    "* Most of the audio files are around 1.8-6.5 seconds long.\n",
    "* There are very few samples below 1.6 seconds and almost none aboce 10.6 seconds length.\n",
    "* There seem to exist two peaks in the mean and max distribution of the signal energy, maybe caused by normalizations of two input datasets?\n",
    "* The signal energy also has a longer tail towards the high end of the signal energy, meaning there will be a large spectrum of louder samples with varying signal energy.\n",
    "* Most of the data seems to be of quite tonal nature. However, there are around 300k samples that are a little noisy and quite a few samples with high noisyness level. (according to spectral flatness)\n",
    "\n",
    "**For all these findings:** Consider checking if the distribution if the same in train, val or try to experimentally determine if it seems to be the same in test. If not, consider adjusting the distribution, e.g., by normalizing the loudness of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b223e-cf2f-4e36-85d0-6bb693883833",
   "metadata": {},
   "source": [
    "## Biases and Embedding-based Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c749144-7a4b-41dc-be90-e88b388c3221",
   "metadata": {},
   "source": [
    "### Interactive Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2466389-fcbd-4d97-9ce6-6180c5f45207",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotlight.show(df.sample(10000), dtype={\"audio_embedding\": Embedding, \"text_embedding\": Embedding, \"audio\": Audio})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73179e27-1686-4f51-83f5-18122c6a3ec5",
   "metadata": {},
   "source": [
    "**Embedding-based Bias Detection**:\n",
    "* There seems to be a slight bias towards male speakers.\n",
    "* ...more biases to be identified via baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cbe3a7-599e-48a0-8b26-faf9562b1dde",
   "metadata": {},
   "source": [
    "## Duplicates and Near Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3be1e-0b6d-4935-8c94-31a8cb338aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact duplicates in sentences\n",
    "print(\"##### Distribution on exact duplicates in sentences #####\")\n",
    "px.histogram(df[\"sentence\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6bde6-d61d-42de-9f5a-772f70ae04e2",
   "metadata": {},
   "source": [
    "**Exact duplicates (Text):**\n",
    "* There are around 350k sentences that are exactly duplicated in the dataset.\n",
    "* 67k samples sentences are duplicated twice.\n",
    "* Few sentences are duplicated significantly more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351de85a-432e-4bcc-af37-04f2cabb7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near duplicates in sentences\n",
    "text_embeddings = np.vstack(df[\"text_embedding\"].sample(20000))\n",
    "kdtree = KDTree(text_embeddings)\n",
    "\n",
    "distances = []\n",
    "for emb in tqdm(text_embeddings):\n",
    "    dist, ind = kdtree.query([emb], k=10)\n",
    "    distances.append(dist[0])\n",
    "\n",
    "distances = np.array(distances)\n",
    "distances = distances[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8faefa9-6340-4e7e-b820-76a46fa1f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(distances.min(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed0259-f8f3-4bd0-a207-b19447671a6a",
   "metadata": {},
   "source": [
    "**Near Duplicates (Text):**\n",
    "* It seems like there are about 2.5% of samples with near or identical duplicates.\n",
    "* Then there are very few samples that are maybe really similar, however they are not a significant portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7079b78-f82d-4835-9999-26d165b0d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near duplicates in audio\n",
    "audio_embeddings = np.vstack(df[\"audio_embedding\"].sample(20000))\n",
    "kdtree = KDTree(audio_embeddings)\n",
    "\n",
    "distances = []\n",
    "for emb in tqdm(audio_embeddings):\n",
    "    dist, ind = kdtree.query([emb], k=10)\n",
    "    distances.append(dist[0])\n",
    "\n",
    "distances = np.array(distances)\n",
    "distances = distances[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc2434-db67-4540-b5dd-e09fab7fa923",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(distances.min(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a70fe-5597-4d25-88e1-5813101fd6ff",
   "metadata": {},
   "source": [
    "**Near Duplicates (Audio):**\n",
    "* It seems like there are no distances close to zero.\n",
    "* This makes near duplicates being present in the audio data unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f6a0f-302d-41a0-ba84-4caa35a24f90",
   "metadata": {},
   "source": [
    "## Outliers/Anomalies/Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b69c7-e9a6-489a-b08f-783f0968c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for calculating outliers for the full dataset you will need around 40GB of RAM.\n",
    "# To decrease the amount of memory needed downsample the data. Of course this will throw away some potential outliers,\n",
    "# however enough will be left to get a feel for typical problematic cases.\n",
    "NUM_SAMPLES = 30000\n",
    "\n",
    "if NUM_SAMPLES is not None:\n",
    "    selected_indices = np.random.choice(np.arange(len(df)), size=NUM_SAMPLES)\n",
    "else:\n",
    "    selected_indices = np.arange(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24cf34-7750-4767-af4f-3f9e03583e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers based on the audio_embedding column.\n",
    "# The library will essentially fit an outlier detection model and search for clusters of data that are more anomal than their parent clusters.\n",
    "sg = SliceGuard()\n",
    "issues = sg.find_issues(df.iloc[selected_indices], [\"audio_embedding\"], min_drop=0.035, min_support=10, drop_reference=\"parent\", precomputed_embeddings={\"audio_embedding\": np.vstack(df[\"audio_embedding\"].iloc[selected_indices])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e753f-6056-4bec-8491-3816da0c8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only the found issues, no other samples.\n",
    "# Remove the parameter if you want to see all data, but beware the interactive report gets jerky above 50k samples.\n",
    "sg.report(non_issue_portion=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61f130-50a1-4a23-82ef-1a78ce6b1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers based on the text_embedding column.\n",
    "# The library will essentially fit an outlier detection model and search for clusters of data that are more anomal than their parent clusters.\n",
    "sg = SliceGuard()\n",
    "sg.find_issues(df, [\"text_embedding\"], min_drop=0.1, min_support=10, precomputed_embeddings={\"text_embedding\": np.vstack(df[\"text_embedding\"])}, drop_reference=\"parent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9453e9-5568-494c-939d-4273a14c5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only the found issues, no other samples.\n",
    "# Remove the parameter if you want to see all data, but beware the interactive report gets jerky above 50k samples.\n",
    "sg.report(non_issue_portion=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9545b-1cb9-492c-b4cd-916d73e044ec",
   "metadata": {},
   "source": [
    "# Free EDA in Spotlight\n",
    "Now it's your turn. Use Spotlight to uncover even more hidden patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e86af2-94b3-48dd-ac5c-12e3cd723ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotlight.show(df, dtype={\"audio_embedding\": Embedding, \"text_embedding\": Embedding, \"audio\": Audio})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
